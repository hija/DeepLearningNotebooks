{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with numpy\n",
    "First attempt to use the logistic regression to classify wheter a patient has cancer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable plotting inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tPrerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Training-Data-Shape: (30, 512)\n",
      "Y-Training-Data-Shape: (1, 512)\n",
      "X-Test-Data-Shape: (30, 57)\n",
      "Y-Test-Data-Shape: (1, 57)\n"
     ]
    }
   ],
   "source": [
    "X_data = data[\"data\"].T #We reshape since we want to have the features column wise\n",
    "Y_data = data[\"target\"]\n",
    "\n",
    "# Normalize X-Data\n",
    "X_data = X_data / np.linalg.norm(X_data, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "# Y_data is a one-dimensional, so we make it a two-dimensional\n",
    "# for better data handling\n",
    "\n",
    "Y_data = Y_data.reshape((1,Y_data.shape[0]))\n",
    "\n",
    "# 90% for training, 10% for testing\n",
    "m_train = int(X_data.shape[1]*0.9)\n",
    "#m_test = X_data.shape[0]-m_train\n",
    "\n",
    "# Split into training and test data\n",
    "X_train_data = X_data[:,:m_train]\n",
    "X_test_data = X_data[:,m_train:]\n",
    "\n",
    "Y_train_data = Y_data[:,:m_train]\n",
    "Y_test_data = Y_data[:,m_train:]\n",
    "\n",
    "# Print out the final shapes\n",
    "print(\"X-Training-Data-Shape:\", X_train_data.shape)\n",
    "print(\"Y-Training-Data-Shape:\", Y_train_data.shape)\n",
    "print(\"X-Test-Data-Shape:\", X_test_data.shape)\n",
    "print(\"Y-Test-Data-Shape:\", Y_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "- Sigmoid function: __Calculates the sigmoid values for an array__\n",
    "- Parameter initialization: __Initializes parameters with 0s, depending on dimension__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    s = 1/(1+np.exp(-X))\n",
    "    return s\n",
    "\n",
    "# Testing the function\n",
    "t = np.array([[1,2],[3,4]])\n",
    "s_t = sigmoid(t)\n",
    "assert(np.array_equal(s_t.round(decimals=2), np.array([[0.73,0.88],[0.95,0.98]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_param(dim):\n",
    "    b = 0 #simple\n",
    "    w = np.zeros((dim,1))\n",
    "    return w,b\n",
    "\n",
    "# Testing the function\n",
    "w,b = init_param(3)\n",
    "assert(b == 0)\n",
    "assert(np.array_equal(np.array([[0],[0],[0]]),w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs and Gradient calculation and Optimization\n",
    "- propagate: __Calculates the cost and the gradients of propagating X with w,b__\n",
    "- optimize: __Optimizes the w and b parameter with the gradients stepwise until good solution found__\n",
    "- predict: __Predicts an Ouput__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(X,Y,w,b):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    cost = -(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    \n",
    "    dz = A-Y\n",
    "    dw = (1/m)*np.dot(X,dz.T)\n",
    "    db = (1/m)*np.sum(dz)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost, dw, db\n",
    "    \n",
    "# Use the method\n",
    "# w,b = init_param(X_train_data.shape[0])\n",
    "# cost, dw, db = propagate(X_train_data,Y_train_data,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    i = 0\n",
    "    while i < num_iterations:\n",
    "        cost,dw,db = propagate(X,Y,w,b)\n",
    "        \n",
    "        if print_cost == True and i % 1000 == 0:\n",
    "            print(\"Iteration:\", i, \"Cost:\", cost)\n",
    "        \n",
    "        # Adjust the parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        i += 1\n",
    "    return w,b\n",
    "\n",
    "# Test\n",
    "w,b = init_param(X_train_data.shape[0])\n",
    "w,b = optimize(w,b,X_train_data,Y_train_data,300000,0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    Y_prediction[A <= 0.5] = 0\n",
    "    Y_prediction[A > 0.5] = 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    return Y_prediction\n",
    "\n",
    "def accuracy(w,b,X,Y):\n",
    "    yhat = predict(w,b,X)\n",
    "    accuracy = 1-(1/Y.shape[1]*np.sum(np.abs(Y-yhat)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training-set: 95.703125%\n",
      "Accuracy on the test-set: 98.2456140351%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on the training-set: \", 100 * accuracy(w,b,X_train_data,Y_train_data), \"%\", sep=\"\")\n",
    "print(\"Accuracy on the test-set: \", 100 * accuracy(w,b,X_test_data,Y_test_data), \"%\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Result\n",
    "The logistic classifier is good at classifying the training-set.\n",
    "If we look at the test-set, it can be shown that no overfitting happens, but that the accuracy of the test-set is even higher than of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
