{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of Linear Regression: 0.77\r\n",
      "Test accuracy of Linear Regression: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression w/ sklearn\n",
    "\n",
    "# Load dataset\n",
    "t = mglearn.datasets.load_boston()\n",
    "X, Y = (t.data, t.target)\n",
    "\n",
    "# Make train/test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Check accuracy\n",
    "train_acc = model.score(X_train, Y_train)\n",
    "test_acc = model.score(X_test, Y_test)\n",
    "print('Train accuracy of Linear Regression: {:.2f}\\r\\nTest accuracy of Linear Regression: {:.2f}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of own implementation: 0.77\r\n",
      "Test accuracy of own implementation: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression w/ Matrix fun\n",
    "\n",
    "class MatrixLinearRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X_ = np.append(np.ones((X.shape[0],1)), X, axis = 1)\n",
    "        tmp1 = np.linalg.inv(np.dot(X_.T, X_))\n",
    "        tmp2 = np.dot(X_.T,Y)\n",
    "        self.betas = np.dot(tmp1, tmp2)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        X_ = np.append(np.ones((X.shape[0],1)), X, axis = 1)\n",
    "        prediction = np.dot(X_, self.betas)\n",
    "        Y_mean = np.mean(Y)\n",
    "        ssr = np.sum((prediction - Y)**2)\n",
    "        ssto = np.sum((Y - Y_mean)**2)\n",
    "        return 1 - ssr / ssto\n",
    "\n",
    "model = MatrixLinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print('Train accuracy of own implementation: {:.2f}\\r\\nTest accuracy of own implementation: {:.2f}'.format(model.score(X_train, Y_train), model.score(X_test, Y_test)))\n",
    "\n",
    "#Notice:\n",
    "#This Regression fails if underdetermined, i.e. with the extended_boston_example from mglearn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizations\n",
    "Regularization is a way of preventing overfitting by adding a penalty. There are three ways (main) ways of regularization.\n",
    "\n",
    "## L1-Regularization\n",
    "The $L_1$-Regularization penalty term is: $\\lambda\\sum_{j=1}^{p}|\\beta_j|$, with $\\beta$ being the weights and $p$ the number of weights. With the $L_1$-Regularization there is a kind of automatically feature selection, since some weights will become $0$, indicating that the feature will not be used.\n",
    "\n",
    "## L2-Regularization\n",
    "The $L_2$-Regularization works by adding the penalty term $\\lambda\\sum_{j=1}^{p}{\\beta_j}^2$. This will not yield in a feature selection, but also all features will be considered.\n",
    "\n",
    "## Elastic Net-Regularization\n",
    "The Elastic Net-Regularization combines the previous two regularization methods. $\\lambda\\sum_{j=1}^{p}|\\beta_j| + \\lambda\\sum_{j=1}^{p}{\\beta_j}^2$ is the penalty this time.\n",
    "\n",
    "## $\\lambda$-Choice\n",
    "Tha $\\lambda$-value determines how much regularization will be applied. If the value is very large, underfitting is likely, since the weights can decrease so much, that they have no real influence on the result anymore. If $\\lambda$ is too low, the regularization has no to little effect. After all, cross validation can be used to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Accuracy of LM: 0.95\n",
      "Test-Accuracy of LM: 0.61\n",
      "\n",
      "Train-Accuracy of Lasso: 0.90\n",
      "Test-Accuracy of Lasso: 0.77\n",
      "\n",
      "Train-Accuracy of Ridge: 0.92\n",
      "Test-Accuracy of Ridge: 0.77\n",
      "\n",
      "Train-Accuracy of ElasticNet: 0.92\n",
      "Test-Accuracy of ElasticNet: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Load a new testdataset to see impact of regularization\n",
    "X, Y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "lmmodel = LinearRegression()\n",
    "lmmodel.fit(X_train, Y_train)\n",
    "\n",
    "# L1-Regularization: Lasso\n",
    "lassomodel = Lasso(alpha=0.01, max_iter=10000)\n",
    "lassomodel.fit(X_train, Y_train)\n",
    "\n",
    "# L2-Regularization: Ridge\n",
    "ridgemodel = Ridge(alpha=0.15)\n",
    "ridgemodel.fit(X_train, Y_train)\n",
    "\n",
    "# Elastic Net\n",
    "netmodel = ElasticNet(alpha=0.001, max_iter=10000)\n",
    "netmodel.fit(X_train, Y_train)\n",
    "\n",
    "print('Train-Accuracy of LM: {:.2f}'.format(lmmodel.score(X_train, Y_train)))\n",
    "print('Test-Accuracy of LM: {:.2f}'.format(lmmodel.score(X_test, Y_test)))\n",
    "print()\n",
    "\n",
    "print('Train-Accuracy of Lasso: {:.2f}'.format(lassomodel.score(X_train, Y_train)))\n",
    "print('Test-Accuracy of Lasso: {:.2f}'.format(lassomodel.score(X_test, Y_test)))\n",
    "print()\n",
    "\n",
    "print('Train-Accuracy of Ridge: {:.2f}'.format(ridgemodel.score(X_train, Y_train)))\n",
    "print('Test-Accuracy of Ridge: {:.2f}'.format(ridgemodel.score(X_test, Y_test)))\n",
    "print()\n",
    "\n",
    "print('Train-Accuracy of ElasticNet: {:.2f}'.format(netmodel.score(X_train, Y_train)))\n",
    "print('Test-Accuracy of ElasticNet: {:.2f}'.format(netmodel.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
